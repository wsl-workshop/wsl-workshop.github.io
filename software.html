<!DOCTYPE html>
<html lang="en-US">
    <head>
        <title>Imperfect Information Learning Software</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="./style.css">
    </head>

<body>
    <section class="page-header">
        <h1 class="project-name">Imperfect Information Learning Software</h1>
        <h2 class="project-tagline">by Imperfect Information Learning Team, RIKEN AIP, and our great collaborators</h2>
    </section>

<section class="main-content">

<h1>Overview</h1>
    <p>
      At <a href="https://aip.riken.jp/labs/generic_tech/imperfect_inf_learn/" target="_blank"><b>Imperfect Information Learning Team</b></a>,
      <a href="https://aip.riken.jp/?lang=en" target="_blank"><b>Center for Advanced Intelligence Project (AIP)</b></a>,
      <a href="https://www.riken.jp/en/" target="_blank"><b>RIKEN</b></a>,
      we are developing reliable and robust <b>machine learning</b> methods/algorithms that can cope with various factors
      such as <b>weak supervision</b>, <b>noisy supervision</b>, and <b>adversarial attacks</b>.
      This page hosts our program codes used in our published papers.
    </p>
    
    <p>The page contains 4 top-level research topics:
        <ul>
            <li><a href="#weaklysupervised"><i>Weakly supervised learning</i></a> is aimed at solving a learning task from only weakly supervised data (e.g., positive and unlabeled data, data with complementary labels, and data with partial labels);</li>
            <li><a href="#labelnoise"><i>Label-noise learning</i></a> is aimed at solving a learning task from possibly mislabeled data (i.e., the dataset for training a standard classifier is a mixture of correctly and incorrectly labeled data);</li>
            <li><a href="#adversarial"><i>Adversarial robustness</i></a> is aimed at improving the robust accuracy of trained models against adversarial attacks (i.e., tiny perturbations applied on the data to flip the model predictions).</li>
            <li>Our published papers that do not fall into the above 3 topics are included in <a href="#other"><i>other topics</i></a>.</li>
        </ul>
    </p>
    
    <p>For more related machine learning methods/algorithms, please check the following pages of our strategic partners:
        <ul>
            <li><a href="http://www.ms.k.u-tokyo.ac.jp/sugi/software.html" target="_blank">
            Sugiyama-Yokoya-Ishida Lab</a> @ The University of Tokyo, led by Prof. 
            <a href="http://www.ms.k.u-tokyo.ac.jp/sugi/" target="_blank">Masashi Sugiyama</a></li>
            
            <li><a href="https://bhanml.github.io/codedata.html" target="_blank">
            Trustworthy Machine Learning Group</a> @ Hong Kong Baptist University, led by Prof. 
            <a href="https://bhanml.github.io/" target="_blank">Bo Han</a></li>
            
            <li><a href="https://tongliang-liu.github.io/code.html" target="_blank">
            Trustworthy Machine Learning Lab</a> @ The University of Sydney, led by Prof. 
            <a href="https://tongliang-liu.github.io/" target="_blank">Tongliang Liu</a></li>
        </ul>
    </p>


<h1><hr>Disclaimer</h1>
    <p><font color="#FF0000">The software available below is free of charge for research and education purposes. However, you must obtain a license from the author(s) to use it for commercial purposes. The software must not be distributed without prior permission of the author(s).</font></p>
    
    <p><font color="#FF0000">The software is supplied "as is" without warranty of any kind, and the author(s) disclaim any and all warranties, including but not limited to any implied warranties of merchantability and fitness for a particular purpose, and any warranties or non infringement. The user assumes all liability and responsibility for use of the software and in no event shall the author(s) be liable for damages of any kind resulting from its use.</font></p>


<h1 id="weaklysupervised"><hr>Weakly supervised learning</h1>
    <p>
      Collecting fully-supervised data is often costly or even impossible.
      Weakly supervised learning is aimed at solving a learning task from only weakly supervised data.
    </p>  
    <h2>Positive-unlabeled learning</h2>
    <p>
      Positive-unlabeled learning is aimed as solving a binary classification problem only from positive and unlabeled data, without negative data.
    </p>
    <ul>
        <li><p><a href="https://github.com/kiryor/nnPUlearning" target="_blank">
        Analysis of learning from positive and unlabeled data</a> (NeurIPS 2014)</p></li>
        
        <li><p><a href="https://github.com/kiryor/nnPUlearning" target="_blank">
        Convex formulation for learning from positive and unlabeled data</a> (ICML 2015)</p></li>
        
        <li><p><a href="https://github.com/nolfwin/PNU" target="_blank">
        Semi-supervised classification based on classification from positive and unlabeled data</a> (ICML 2017)</p></li>
        
        <li><p><a href="https://github.com/kiryor/nnPUlearning" target="_blank">
        Positive-unlabeled learning with non-negative risk estimator</a> (NeurIPS 2017)</p></li>
        
        <li><p><a href="https://github.com/nolfwin/PNU" target="_blank">
        Semi-supervised AUC optimization based on positive-unlabeled learning</a> (Machine Learning 2018)</p></li>
        
        <li><p><a href="https://github.com/cyber-meow/PUbiasedN" target="_blank">
        Classification from positive, unlabeled and biased negative data</a> (ICML 2019)</p></li>
        
        <li><p><a href="https://github.com/alonjacovi/document-set-expansion-pu" target="_blank">
        Scalable evaluation and improvement of document set expansion via neural positive-unlabeled learning</a> (EACL 2021)</p></li>
    </ul>
    
    <h2>Unlabeled-unlabeled learning</h2>
    <p>
      Unlabeled-unlabeled learning is aimed as solving a binary classification problem only from two sets of unlabeled data with different class priors.
    </p>
    <ul>
        <li><p><a href="https://github.com/lunanbit/UUlearning" target="_blank">
        On the minimal supervision for training any binary classifier from only unlabeled data</a> (ICLR 2019)</p></li>
        
        <li><p>Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach (AISTATS 2020)</p></li>
        
        <li><p><a href="https://github.com/nolfwin/symloss-ber-auc" target="_blank">
        On symmetric losses for learning from corrupted labels</a> (ICML 2020)</p></li>
                
        <li><p><a href="https://github.com/leishida/Um-Classification" target="_blank">
        Binary Classification from multiple unlabeled datasets via surrogate set classification</a> (ICML 2021)</p></li>
    </ul>
    
    <h2>Complementary-label learning</h2>
    <p>
      Complementary-label learning is aimed at training a multi-class classifier only from complementarily labeled data
      (a complementary label incidates a class which a patter does NOT belong to).
    </p>
    <ul>
        <li><p><a href="https://github.com/takashiishida/comp" target="_blank">
        Learning from complementary labels</a> (NeurIPS 2017)</p></li>
        
        <li><p><a href="https://github.com/takashiishida/comp" target="_blank">
        Complementary-label learning for arbitrary losses and models</a> (ICML 2019)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/LMCL.rar" target="_blank">
        Learning with multiple complementary labels</a> (ICML 2020)</p></li>
        
        <li><p>Unbiased risk estimators can mislead: A case study of learning with complementary labels (ICML 2020)</p></li>
    </ul>
    
    <h2>Partial-label learning</h2>
    <p>
      Partial-label learning is aimed at training a multi-class classifier only from partially labeled data
      (a partial label incidates a set of class labels one of which is the true one).
    </p>
    <ul>
        <li><p><a href="https://lfeng-ntu.github.io/Codes/LMCL.rar" target="_blank">
        Learning with multiple complementary labels</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/Lvcrezia77/PRODEN" target="_blank">
        Progressive identification of true labels for partial-label learning</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/RCCC.rar" target="_blank">
        Provably consistent partial-label learning</a> (NeurIPS 2020)</p></li>
    </ul>
    
    <h2>Pairwise learning</h2>
    <p>
      Pairwise learning is aimed at solving a classification problem from pairwise similarities/dissimilarities.
    </p>
    <ul>
        <li><p><a href="https://github.com/levelfour/SU_Classification" target="_blank">
        Classification from pairwise similarity and unlabeled data</a> (ICML 2018)</p></li>
        
        <li><p>Uncoupled regression from pairwise comparison data (NeurIPS 2019)</p></li>
        
        <li><p>Learning from similarity-confidence data (ICML 2021)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/Pcomp.zip" target="_blank">
        Pointwise binary classification with pairwise confidence comparisons</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/SDMIL.zip" target="_blank">
        Multiple-instance learning from similar and dissimilar bags</a> (KDD 2021)</p></li>
    </ul>
    
    <h2>Other</h2>
    <ul>
        <li><p><a href="https://github.com/takashiishida/pconf" target="_blank">
        Binary classification from positive-confidence data</a> (NeurIPS 2018)</p></li>
        
        <li><p><a href="https://github.com/TongtongFANG/DIW" target="_blank">
        Rethinking importance weighting for deep learning under distribution shift</a> (NeurIPS 2020)</p></li>
    </ul>


<h1 id="labelnoise"><hr>Label-noise learning</h1>
    <h2>Loss correction for class-conditional noise</h2>
    <ul>
        <li><p><a href="https://github.com/bhanML/Masking" target="_blank">
        Masking: A new perspective of noisy supervision</a> (NeurIPS 2018)</p></li>
        
        <li><p><a href="https://github.com/xiaoboxia/T-Revision" target="_blank">
        Are anchor points really indispensable in label-noise learning?</a> (NeurIPS 2019)</p></li>
        
        <li><p><a href="https://github.com/a5507203/Dual-T" target="_blank">
        Dual T: Reducing estimation error for transition matrix in label-noise learning</a> (NeurIPS 2020)</p></li>
        
        <li><p><a href="https://github.com/YivanZhang/lio" target="_blank">
        Learning noise transition matrix from only noisy labels via total variation regularization</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/xuefeng-li1/Provably-end-to-end-label-noise-learning-without-anchor-points" target="_blank">
        Provably end-to-end label-noise learning without anchor points</a> (ICML 2021)</p></li>

    </ul>
    
    <h2>Sample selection/reweighting for class-conditional noise</h2>
    <ul>
        <li><p><a href="https://github.com/bhanML/Co-teaching" target="_blank">
        Co-teaching: Robust training of deep neural networks with extremely noisy labels</a> (NeurIPS 2018)</p></li>
        
        <li><p><a href="https://github.com/xingruiyu/coteaching_plus" target="_blank">
        How does disagreement help generalization against label corruption?</a> (ICML 2019)</p></li>
        
        <li><p><a href="https://github.com/AutoML-Research/S2E" target="_blank">
        Searching to exploit memorization effect in learning with noisy labels</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/TongtongFANG/DIW" target="_blank">
        Rethinking importance weighting for deep learning under distribution shift</a> (NeurIPS 2020)</p></li>
    </ul>
    
    <h2>Other techniques for class-conditional noise</h2>
    <ul>
        <li><p><a href="https://github.com/bhanML/SIGUA" target="_blank">
        SIGUA: Forgetting may make learning with noisy labels more robust</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/scifancier/Class2Simi" target="_blank">
        Class2Simi: A noise reduction perspective on learning with noisy labels</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/tmllab/PES" target="_blank">
        Understanding and improving early stopping for learning with noisy labels</a> (NeurIPS 2021)</p></li>
    </ul>
    
    <h2>Instance-dependent noise</h2>
    <ul>
        <li><p><a href="https://github.com/xiaoboxia/Part-dependent-label-noise" target="_blank">
        Part-dependent label noise: Towards instance-dependent label noise</a> (NeurIPS 2020)</p></li>
        
        <li><p><a href="https://github.com/QizhouWang/instance-dependent-label-noise" target="_blank">
        Tackling instance-dependent label noise via a universal probabilistic model</a> (AAAI 2021)</p></li>
        
        <li><p><a href="https://github.com/antoninbrthn/CSIDN" target="_blank">
        Confidence scores make instance-dependent label-noise learning possible</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/a5507203/IDLN" target="_blank">
        Instance-dependent label-noise learning under a structural causal model</a> (NeurIPS 2021)</p></li>
    </ul>


<h1 id="adversarial"><hr>Adversarial robustness</h1>
    <p>
        When we deploy models trained by standard supervised learning, they work well on <i>natural</i> test data.
        However, those models cannot handle <i>adversarial</i> test data (also known as <i>adversarial examples</i>) that are algorithmically generated by <i>adversarial attacks</i>.
        An adversarial attack is an algorithm which applies specially designed tiny perturbations on natural data to transform them into adversarial data, in order to mislead a trained model and let it give wrong predictions.
        <i>Adversarial robustness</i> is aimed at improving the robust accuracy of trained models against adversarial attacks.
    </p>
    
    <h2>Algorithm perspective</h2>
    
    
    <ul>
        <li><p><a href="https://github.com/zjfheart/Friendly-Adversarial-Training" target="_blank">
        Attacks which do not kill training make adversarial learning stronger</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/zjfheart/Geometry-aware-Instance-reweighted-Adversarial-Training" target="_blank">
        Geometry-aware instance-reweighted adversarial training</a> (ICLR 2021)</p></li>
        
        <li><p><a href="https://github.com/QizhouWang/MAIL" target="_blank">
        Probabilistic margins for instance reweighting in adversarial training</a> (NeurIPS 2021)</p></li>
    </ul>
    
    <h2>Model perspective</h2>
    <ul>
        <li><p><a href="https://github.com/HanshuYAN/CIFS" target="_blank">
        CIFS: Improving adversarial robustness of CNNs via channel-wise Importance-based feature selection</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/d12306/dsnet" target="_blank">
        Learning diverse-structured networks for adversarial robustness</a> (ICML 2021)</p></li>
    </ul>
    
    <h2>Detection perspective</h2>
    <ul>
        <li><p><a href="https://github.com/fengliu90/SAMMD" target="_blank">
        Maximum mean discrepancy test is aware of adversarial attacks</a> (ICML 2021)</p></li>
    </ul>


<h1 id="other"><hr>Other</h1>
    <ul>
        <li><p><a href="http://parnec.nuaa.edu.cn/huangsj/alipy/" target="_blank">
        Active feature acquisition with supervised matrix completion</a> (KDD 2018)</p></li>

        <li><p><a href="https://github.com/voot-t/guide-actor-critic" target="_blank">
        Guide actor-critic for continuous control</a> (ICLR2018)</p></li>

        <li><p><a href="https://github.com/takashiishida/flooding" target="_blank">
        Do we need zero training loss after achieving zero training error?</a> (ICML 2020)</p></li>

        <li><p><a href="https://github.com/diadochos/few-shot-domain-adaptation-by-causal-mechanism-transfer" target="_blank">
        Few-shot domain adaptation by causal mechanism transfer</a> (ICML 2020)</p></li>

        <li><p><a href="https://github.com/voot-t/vild_code" target="_blank">
        Variational imitation learning with diverse-quality demonstrations</a> (ICML 2020)</p></li>

        <li><p><a href="https://github.com/voot-t/ril_co" target="_blank">
        Robust imitation learning from noisy demonstrations</a> (AISTATS 2021)</p></li>

        <li><p>Large-margin contrastive learning with distance polarization regularizer (ICML 2021)</p></li>
      
        <li><p><a href="https://github.com/diadochos/incorporating-causal-graphical-prior-knowledge-into-predictive-modeling-via-simple-data-augmentation" target="_blank">
        Incorporating causal graphical prior knowledge into predictive modeling via simple data augmentation</a> (UCI 2021)</p></li>
        

        
    </ul>

</section>

</body></html>
