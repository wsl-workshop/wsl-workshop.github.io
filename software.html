<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Imperfect Information Learning Software</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="./style.css">
  </head>

 <body>
    <section class="page-header">
      <h1 class="project-name">Imperfect Information Learning Software</h1>
      <h2 class="project-tagline">Partially from Imperfect Information Learning Team, RIKEN Center for Advanced Intelligence Project</h2>
    </section>

<section class="main-content">

<h1>Overview</h1>
    <p>There should be an overview.</p>
    <p><a href="#weaklysupervised">weakly supervised learning</a></p>
    <p><a href="#labelnoise">label-noise learning</a></p>
    <p><a href="#adversarial">adversarial training</a></p>
    <p><a href="#other">other</a></p>
    
<h1 id="weaklysupervised"><hr>Weakly supervised learning</h1>
    <h2>Complementary-label learning</h2>
    <ul>
        <li><p><a href="https://github.com/takashiishida/comp" target="_blank">
        Learning from complementary labels</a> (NeurIPS 2017)</p></li>
        
        <li><p><a href="https://github.com/takashiishida/comp" target="_blank">
        Complementary-label learning for arbitrary losses and models</a> (ICML 2019)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/LMCL.rar" target="_blank">
        Learning with multiple complementary labels</a> (ICML 2020)</p></li>
        
        <li><p>Unbiased risk estimators can mislead: A case study of learning with complementary labels (ICML 2020)</p></li>
    </ul>
    
    <h2>Pairwise learning</h2>
    <ul>
        <li><p><a href="https://github.com/levelfour/SU_Classification" target="_blank">
        Classification from pairwise similarity and unlabeled data</a> (ICML 2018)</p></li>
        
        <li><p>Uncoupled regression from pairwise comparison data (NeurIPS 2019)</p></li>
        
        <li><p>Learning from similarity-confidence data (ICML 2021)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/Pcomp.zip" target="_blank">
        Pointwise binary classification with pairwise confidence comparisons</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/SDMIL.zip" target="_blank">
        Multiple-instance learning from similar and dissimilar bags</a> (KDD 2021)</p></li>
    </ul>
    
    <h2>Partial-label learning</h2>
    <ul>
        <li><p><a href="https://lfeng-ntu.github.io/Codes/LMCL.rar" target="_blank">
        Learning with multiple complementary labels</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/Lvcrezia77/PRODEN" target="_blank">
        Progressive identification of true labels for partial-label learning</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://lfeng-ntu.github.io/Codes/RCCC.rar" target="_blank">
        Provably consistent partial-label learning</a> (NeurIPS 2020)</p></li>
    </ul>
    
    <h2>Positive-unlabeled learning</h2>
    <ul>
        <li><p><a href="https://github.com/kiryor/nnPUlearning" target="_blank">
        Analysis of learning from positive and unlabeled data</a> (NeurIPS 2014)</p></li>
        
        <li><p><a href="https://github.com/kiryor/nnPUlearning" target="_blank">
        Convex formulation for learning from positive and unlabeled data</a> (ICML 2015)</p></li>
        
        <li><p><a href="https://github.com/nolfwin/PNU" target="_blank">
        Semi-supervised classification based on classification from positive and unlabeled data</a> (ICML 2017)</p></li>
        
        <li><p><a href="https://github.com/kiryor/nnPUlearning" target="_blank">
        Positive-unlabeled learning with non-negative risk estimator</a> (NeurIPS 2017)</p></li>
        
        <li><p><a href="https://github.com/nolfwin/PNU" target="_blank">
        Semi-supervised AUC optimization based on positive-unlabeled learning</a> (Machine Learning 2018)</p></li>
        
        <li><p><a href="https://github.com/cyber-meow/PUbiasedN" target="_blank">
        Classification from positive, unlabeled and biased negative data</a> (ICML 2019)</p></li>
        
        <li><p><a href="https://github.com/alonjacovi/document-set-expansion-pu" target="_blank">
        Scalable evaluation and improvement of document set expansion via neural positive-unlabeled learning</a> (EACL 2021)</p></li>
    </ul>
    
    <h2>Unlabeled-unlabeled learning</h2>
    <ul>
        <li><p><a href="https://github.com/lunanbit/UUlearning" target="_blank">
        On the minimal supervision for training any binary classifier from only unlabeled data</a> (ICLR 2019)</p></li>
        
        <li><p>Mitigating overfitting in supervised classification from two unlabeled datasets: A consistent risk correction approach (AISTATS 2020)</p></li>
        
        <li><p><a href="https://github.com/leishida/Um-Classification" target="_blank">
        Binary Classification from multiple unlabeled datasets via surrogate set classification</a> (ICML 2021)</p></li>
    </ul>
    
    <h2>Other</h2>
    <ul>
        <li><p><a href="https://github.com/takashiishida/pconf" target="_blank">
        Binary classification from positive-confidence data</a> (NeurIPS 2018)</p></li>
        
        <li><p><a href="https://github.com/TongtongFANG/DIW" target="_blank">
        Rethinking importance weighting for deep learning under distribution shift</a> (NeurIPS 2020)</p></li>
    </ul>

<h1 id="labelnoise"><hr>Label-noise learning</h1>
    <ul>
        <li><p><a href="https://github.com/bhanML/Co-teaching" target="_blank">
        Co-teaching: Robust training of deep neural networks with extremely noisy labels</a> (NeurIPS 2018)</p></li>
        
        <li><p><a href="https://github.com/bhanML/Masking" target="_blank">
        Masking: A new perspective of noisy supervision</a> (NeurIPS 2018)</p></li>
        
        <li><p><a href="https://github.com/xingruiyu/coteaching_plus" target="_blank">
        How does disagreement help generalization against label corruption</a> (ICML 2019)</p></li>
        
        <li><p><a href="https://github.com/xiaoboxia/T-Revision" target="_blank">
        Are anchor points really indispensable in label-noise learning?</a> (NeurIPS 2019)</p></li>
        
        <li><p><a href="https://github.com/AutoML-Research/S2E" target="_blank">
        Searching to exploit memorization effect in learning with noisy labels</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/bhanML/SIGUA" target="_blank">
        SIGUA: Forgetting may make learning with noisy labels more robust</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/a5507203/Dual-T" target="_blank">
        Dual T: Reducing estimation error for transition matrix in label-noise learning</a> (NeurIPS 2020)</p></li>
        
        <li><p><a href="https://github.com/xiaoboxia/Part-dependent-label-noise" target="_blank">
        Part-dependent label noise: Towards instance-dependent label noise</a> (NeurIPS 2020)</p></li>
        
        <li><p><a href="https://github.com/TongtongFANG/DIW" target="_blank">
        Rethinking importance weighting for deep learning under distribution shift</a> (NeurIPS 2020)</p></li>
        
        <li><p><a href="https://github.com/QizhouWang/instance-dependent-label-noise" target="_blank">
        Tackling instance-dependent label noise via a universal probabilistic model</a> (AAAI 2021)</p></li>
        
        <li><p><a href="https://github.com/scifancier/Class2Simi" target="_blank">
        Class2Simi: A noise reduction perspective on learning with noisy labels</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/antoninbrthn/CSIDN" target="_blank">
        Confidence scores make instance-dependent label-noise learning possible</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/YivanZhang/lio" target="_blank">
        Learning noise transition matrix from only noisy labels via total variation regularization</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/xuefeng-li1/Provably-end-to-end-label-noise-learning-without-anchor-points" target="_blank">
        Provably end-to-end label-noise learning without anchor points</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/a5507203/IDLN" target="_blank">
        Instance-dependent label-noise learning under a structural causal model</a> (NeurIPS 2021)</p></li>
        
        <li><p><a href="https://github.com/tmllab/PES" target="_blank">
        Understanding and improving early stopping for learning with noisy labels</a> (NeurIPS 2021)</p></li>
    </ul>

<h1 id="#adversarial"><hr>Adversarial training</h1>
    <ul>
        <li><p><a href="https://github.com/zjfheart/Friendly-Adversarial-Training" target="_blank">
        Attacks Which Do Not Kill Training Make Adversarial Learning Stronger</a> (ICML 2020)</p></li>
        
        <li><p><a href="https://github.com/zjfheart/Geometry-aware-Instance-reweighted-Adversarial-Training" target="_blank">
        Geometry-aware instance-reweighted adversarial training</a> (ICLR 2021)</p></li>
        
        <li><p><a href="https://github.com/HanshuYAN/CIFS" target="_blank">
        CIFS: Improving adversarial robustness of CNNs via channel-wise Importance-based feature selection</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/d12306/dsnet" target="_blank">
        Learning diverse-structured networks for adversarial robustness</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/fengliu90/SAMMD" target="_blank">
        Maximum mean discrepancy is aware of adversarial attacks</a> (ICML 2021)</p></li>
        
        <li><p><a href="https://github.com/QizhouWang/MAIL" target="_blank">
        Probabilistic margins for instance reweighting in adversarial training</a> (NeurIPS 2021)</p></li>
        
    </ul>

<h1 id="#other"><hr>Other</h1>
    <ul>
        <li><p><a href="http://parnec.nuaa.edu.cn/huangsj/alipy/" target="_blank">
        Active feature acquisition with supervised matrix completion</a> (KDD 2018)</p></li>
        
        <li><p><a href="https://github.com/takashiishida/flooding" target="_blank">
        Do we need zero training loss after achieving zero training error?</a> (ICML 2020)</p></li>
        
        <li><p>Large-margin contrastive learning with distance polarization regularizer (ICML 2021)</p></li>
    </ul>





</section>
</body></html>
