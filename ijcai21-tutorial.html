<!DOCTYPE html>
<!-- saved from url=(0031)https://wsl-workshop.github.io/ -->
<html lang="en-US"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Welcome to IJCAI2021 Learning with Noisy Supervision Tutorial</title>
<meta name="generator" content="Jekyll v3.8.5">
<meta property="og:title" content="Welcome to SDM2020 Weakly-supervised and Unsupervised Learning Workshop">
<meta property="og:locale" content="en_US">
<meta name="description" content="SDM2020 Weakly-supervised and Unsupervised Learning Workshop">
<meta property="og:description" content="SDM2020 Weakly-supervised and Unsupervised Learning Workshop">
<link rel="canonical" href="https://wsl-workshop.github.io/">
<meta property="og:url" content="https://wsl-workshop.github.io/">
<meta property="og:site_name" content="wsl-workshop.github.io">
<script type="application/ld+json">
{"@type":"WebSite","url":"https://wsl-workshop.github.io/","name":"wsl-workshop.github.io","headline":"Welcome to SDM2020 Weakly-supervised and Unsupervised Learning Workshop","description":"ACML19 Weakly-supervised Learning Workshop","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="./style.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">IJCAI2021 Tutorial</h1>
      <h2 class="project-tagline">Learning with Noisy Supervision, Aug, Online</h2>  

    </section>

    <section class="main-content">
<!--       <h2 id="May 9th, Cincinnati, Ohio, United States">May 9th, Cincinnati, Ohio, United States
</h2> -->

<!-- <h1 id="topic-summary">Zoom Recordings</h1>
<p>Check <a href="https://hkbu.zoom.us/rec/share/Zn_Cetck7GfshdlGYE160NcBsQrx2TfbmimWSjDUPdZV5L3LSC8byx3Vgj1-WmNC.WKlJu8XhID9_OC-H">Workshop Recordings</a></p> -->


<h1 id="topic-summary">Abstract</h1>

<p>Machine learning should benefit to the whole world, especially for developing countries in Africa and Asia. When dataset sizes grow bigger, it is laborious and expensive to obtain clean supervision, especially for developing countries. As a result, the volume of noisy supervision becomes enormous, e.g., web-scale image and speech data with noisy labels. However, standard machine learning assumes that the supervised information is fully clean and intact. Therefore, noisy data harms the performance of most of the standard learning algorithms, and sometimes even makes existing algorithms break down. There are a brunch of theories and approaches proposed to deal with noisy data. As far as we know, label-noise learning spans over two important ages in machine learning: statistical learning (i.e., shallow learning) and deep learning. In the age of statistical learning, label-noise learning focused on designing noise-tolerant losses or unbiased risk estimators. Nonetheless, in the age of deep learning, label-noise learning has more options to combat with noisy labels, such as designing biased risk estimators or leveraging memorization effects of deep networks. In this tutorial, we summarize the foundations and go through the most recent noisy-supervision-tolerant techniques. By participating the tutorial, the audience will gain a broad knowledge of label-noise learning from the viewpoint of statistical learning theory, deep learning, detailed analysis of typical algorithms and frameworks, and their real-world applications in industry.</p>

<h1 id="tentative-schedule">Schedule</h1>

<p>The workshop will be held at August, 2021</p>
<!-- <p>The workshop is totally free online. Check our workshop recordings</p> -->

<table>
  <thead>
    <tr>
      <th>Time</th>
      <th>Event</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>14:00-14:15</td>
      <td><strong>Part 1</strong></td>
    </tr>
	<tr>
      <td>&nbsp;</td>
      <td><strong>Title</strong>: Overview of Learning with Noisy Supervision</td>
    </tr>
	<tr>
      <td>&nbsp;</td>
      <td><strong>Speaker</strong>: Masashi Sugiyama</td>
    </tr>
    <tr>
      <td>14:15-15:15</td>
      <td><strong>Part 2</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><strong>Title</strong>: Statistical Learning with Noisy Supervision</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
        <td><strong>Speaker</strong>: Tongliang Liu</td>
    </tr>
    <tr>
      <td>15:15-16:15</td>
      <td><strong>Part 3</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><strong>Title</strong>: Deep Learning with Noisy Supervision</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
        <td><strong>Speaker</strong>: Bo Han</td>
    </tr>
    <tr>
      <td>16:15-17:15</td>
      <td><strong>Part 4</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><strong>Title</strong>: Automated Learning from Noisy Supervision</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
        <td><strong>Speaker</strong>: Quanming Yao</td>
    </tr>
	<tr>
      <td>17:15-17:30</td>
      <td><strong>Part 5</strong></td>
    </tr>
    <tr>
      <td>&nbsp;</td>
      <td><strong>Title</strong>: From Noisy Supervision to other Weak Supervisions</td>
    </tr>
    <tr>
      <td>&nbsp;</td>
        <td><strong>Speaker</strong>: Gang Niu</td>
    </tr>
  </tbody>
</table>

<h1 id="topic-description">Slides</h1>

<p>The slides can be found here.</p>

<h1 id="organizers">Organizers</h1>

<p><a href="https://bhanml.github.io/">Bo Han</a>, Hong Kong Baptist University, Hong Kong SAR, China.</p>

<p><a href="https://tongliang-liu.github.io/">Tongliang Liu</a>, The University of Sydney, Australia.</p>

<p><a href="http://www.cse.ust.hk/~qyaoaa/">Quanming Yao</a>, Tsinghua University, China.</p>

<p><a href="https://niug1984.github.io/">Gang Niu</a>, RIKEN, Japan.</p>

<p><a href="http://www.ms.k.u-tokyo.ac.jp/sugi/">Masashi Sugiyama</a>, RIKEN / University of Tokyo, Japan.</p>

<h1 id="sponsors">References</h1>
<p>Due to the space limitation, we only list highly-related papers. The full reference list can be found <a href="https://arxiv.org/pdf/2011.04406.pdf">here</a>.</p>
<p>
1. B. Han, Q. Yao, T. Liu, G. Niu, I.W. Tsang, J.T. Kwok and M. Sugiyama. A Survey of Label-noise Representation Learning: Past, Present and Future. arXiv preprint arXiv:2011.04406, 2020.<br>
2. N. Natarajan, I.S. Dhillon, P. K. Ravikumar and A. Tewari. Learning with Noisy Labels. In NeurIPS, 2013.<br>
3. S. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan and A. Rabinovich. Training Deep Neural Networks on Noisy Labels with Bootstrapping. In ICLR Workshop, 2015.<br>
4. T. Liu and D. Tao. Classification with Noisy Labels by Importance Reweighting. IEEE Transactions on Pattern Analysis and MachineIntelligence, 38(3): 447-461, 2015.<br>
5. G. Patrini, A. Rozza, A. K. Menon, R. Nock and L. Qu. Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach. In CVPR, 2017.<br>
6. L. Jiang, Z. Zhou, T. Leung, L.-J. Li and L. Fei-Fei. Mentornet: Learning Data-driven Curriculum for very Deep Neural Networks on Corrupted Labels. In ICML, 2018.<br>
7. B. Han, Q. Yao, X. Yu, G. Niu, M. Xu, W. Hu, I.W. Tsang and M. Sugiyama. Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels. In NeurIPS, 2018.<br>
8. B. Han, J. Yao, G. Niu, M. Zhou, I.W. Tsang, Y. Zhang and M. Sugiyama. Masking: A New Perspective of Noisy Supervision. In NeurIPS, 2018.<br>
9. Q. Yao, M. Wang. Taking Human out of Learning Applications: A Survey on Automated Machine Learning. 
arXiv preprint arXiv:1810.13306. 2018<br>
10. X. Yu, B. Han, J. Yao, G. Niu, I.W. Tsang and M. Sugiyama. How does Disagreement Help Generalization against Label Corruption? In ICML, 2019.<br>
11. X. Xia, T. Liu, N. Wang, B. Han, C. Gong, G. Niu, and M. Sugiyama. Are Anchor Points Really Indispensable in Label-Noise Learning? In NeurIPS, 2019.<br>
12. J. Li, R. Socher, S. Hoi. DivideMix: Learning with Noisy Labels as Semi-supervised Learning. In ICLR, 2020.<br>
13. Y. Yao, T. Liu, B. Han, M. Gong, J. Deng, G. Niu, and M. Sugiyama. Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning. In NeurIPS, 2020.<br>
14. X. Xia, T. Liu, B. Han, N. Wang, M. Gong, H. Liu, G. Niu, D. Tao, and M. Sugiyama. Part-dependent Label Noise: Towards Instance-dependent Label Noise. In NeurIPS, 2020.<br>
15. J. Cheng, T. Liu, K. Rao, and D. Tao. Learning with Bounded Instance-and Label-dependent Label Noise.
In ICML, 2020.<br>
16. B. Han, G. Niu, X. Yu, Q. Yao, X. Miao, I.W. Tsang and M. Sugiyama. SIGUA: Forgetting May Make Learning with Noisy Labels More Robust. In ICML, 2020.<br>
17. Y. Zhang, Q. Yao, L. Chen. Interstellar: Searching Recurrent Architecture for Knowledge Graph Embedding. 
In NeurIPS, 2020.<br>
18. Q. Yao, H. Yang, B. Han, G. Niu, J. Kwok. Searching to Exploit Memorization Effect in Learning from Noisy Labels. In ICML, 2020.<br>
19. A. K. Menon, A. S. Rawat, S. J. Reddi and S. Kumar. Can Gradient Clipping Mitigate Label Noise? In ICLR, 2020.<br>
20. X. Ma, H. Huang, Y. Wang, S. Romano, S. Erfani and J. Bailey. Normalized Loss Functions for Deep Learning with Noisy Labels. In ICML, 2020.<br>
21. Q. Yao, J. Xu, W. Tu, Z. Zhu. Efficient Neural Architecture Search via Proximal Iterations. In AAAI, 2020.<br>
22. H. Cheng, Z. Zhu, X. Li, Y. Gong, X. Sun and Y. Liu. Learning with Instance-dependent Label Noise: A Sample Sieve Approach. In ICLR, 2021.<br>
</p>

      <footer class="site-footer">
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com/">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  

</body></html>

